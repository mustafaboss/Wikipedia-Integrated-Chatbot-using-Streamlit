{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58edcb55",
   "metadata": {},
   "source": [
    "# Search Engine with Tools and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5c9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arxiv -- Research\n",
    "\n",
    "## Tools creation\n",
    "\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddead76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Used the inbuilt tool for wikipedia\n",
    "\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)\n",
    "\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627e66c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=250)\n",
    "arxiv = ArxivQueryRun(api_wrapper = api_wrapper_arxiv)\n",
    "\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f3747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki,arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709cbb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\xpert computers\\AppData\\Local\\Temp\\ipykernel_5164\\3468779532.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\xpert computers\\Desktop\\class 31\\Class31\\my_new_env10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Custom tools [RAG Tool]\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c0d08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020644E3B640>, search_kwargs={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "\n",
    "docs= loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "vectordb = FAISS.from_documents(documents,embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e6ef6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever = retriever,\n",
    "    name= \"langsmith_search\",\n",
    "    description = \"Search any information about Langchain\")\n",
    "\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c000deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35479a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\xpert computers\\\\Desktop\\\\class 31\\\\Class31\\\\my_new_env10\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='langsmith_search', description='Search any information about Langchain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000002062752D3F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020644E3B640>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000002062752D480>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020644E3B640>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfcabd",
   "metadata": {},
   "source": [
    "### Conculsion to this :\n",
    "\n",
    "- Tool Creation = How to build or retrieve tools from wikipedia or arxiv by installing specific libraries and wrapping them in Langchain's convenience classes.\n",
    "\n",
    "- Custom Tools = How to load your own data ( like a webpage) and turn it into a retriever, then wrap that retriever as a tool.\n",
    "\n",
    "- Preparing for Agents = We're setting the stage so that in the next part we'll integrate these tools with an LLM or Agen. The agent can automatiocally pick which tool to use based on the question, giving a more powerful \"search engine\" experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd6ff7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run all this tools with Agents and LLM Models\n",
    "\n",
    "## Tools, LLM -- > AgentExecutor\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a17bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xpert computers\\Desktop\\class 31\\Class31\\my_new_env10\\lib\\site-packages\\langsmith\\client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt= hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87da875b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_tools_agent' from 'langchain.agents' (c:\\Users\\xpert computers\\Desktop\\class 31\\Class31\\my_new_env10\\lib\\site-packages\\langchain\\agents\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_tools_agent\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m create_tools_agent(llm\u001b[38;5;241m=\u001b[39mllm, tools\u001b[38;5;241m=\u001b[39mtools, prompt\u001b[38;5;241m=\u001b[39mprompt)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'create_tools_agent' from 'langchain.agents' (c:\\Users\\xpert computers\\Desktop\\class 31\\Class31\\my_new_env10\\lib\\site-packages\\langchain\\agents\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tools_agent\n",
    "\n",
    "agent = create_tools_agent(llm=llm, tools=tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent Executor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee24445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011b4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7890a4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
